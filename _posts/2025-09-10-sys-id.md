---
layout: post
title: System Identification of a DC Motor
date: 2025-09-10 11:12:00-0400
description: White- and black-box approaches to motor control
tags: 
categories: 
thumbnail: assets/img/sys-id.jpg
related_posts: false
---

This project investigates system identification of a DC motor simulator using both analytical and data-driven methods. A white-box state-space model was first derived to establish the expected dynamics, step and frequency responses were measured, and then black-box methods were trained on the data to model and predict the behavior of the full electromechanical system.

---

### White Box Approach

An electric DC motor is connected to a wheel with linear damping $$ c $$ and inertia $$ J $$. The motor generates a torque $$ \tau $$ with some angular velocity $$ \omega $$ which is related to the back emf $$ v_{emf} $$ by the motor constant $$ K_v $$. The motor converts electrical power to mechanical power with efficiency $$ \eta $$, with the equations:

$$
\begin{aligned}
\omega = K_v v_{emf} \quad\quad\quad \eta = \frac{P_{out}}{P_{in}}
\end{aligned}
$$

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/motor.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A DC motor connected to a wheel, the overarching schematic for this system identification analysis.
</div>

Using the efficiency equation with mechanical and electrical power, the relation between current $$ i_m $$ and torque $$ \tau $$ is given by:

$$
\begin{aligned}
P_{out}&=\eta P_{in}\\
\tau \omega &= \eta i_m v_{emf}\\
\tau K_v v_{emf} &= \eta i_m v_{emf}\\
\frac{K_v}{\eta} \tau &= i_m
\end{aligned}
$$

Applying Newton's law of rotation to the system, the mechanical equation of motion is $$ J \dot{\omega} = \tau - c \omega $$.

Applying Kirchoff's current and voltage laws with the current $$ i_m $$, the electrical equation of motion is $$ v_{in} = v_{emf} + R_m i_m $$.

Combining both equations of motion with the given relations:

$$
\begin{aligned}
J\dot{\omega}&=\frac{\eta}{K_v}i_m-c\omega\\
J\dot{\omega}&=\frac{\eta}{R_mK_v}(v-v_{emf})-c\omega\\
J\dot{\omega}&=\frac{\eta}{R_mK_v}(v_{in}-\frac{\omega}{K_v})-c\omega\\
J\dot{\omega}&=\frac{\eta}{R_mK_v}v_{in}-(c+\frac{\eta}{R_mK_v^2})\omega
\end{aligned}
$$

In state-space form, the system is represented by:

$$
\dot{x}=\begin{bmatrix}\dot{\omega}\\\dot{\theta}\end{bmatrix}=\begin{bmatrix}\frac{-\eta}{JR_mK_v^2}-\frac{c}{J}&0\\1&0\end{bmatrix}\begin{bmatrix}\omega\\\theta\end{bmatrix}+\begin{bmatrix}\frac{\eta}{JR_mK_v}\\0\end{bmatrix}v_{in}=Ax+Bu
$$

Using these matrices, the analytical solution $$ x(t) $$ given an input $$ u(t) $$ is formulated as:

$$
x(t)=e^{At}x_0+\int_0^t e^{A(t-\tau)}Bu(\tau)d\tau
$$

For the next analyses, parameters are chosen. The resistance of the motor coil is $$ R_m = 2 \Omega $$, the moment of inertia of the wheel is $$ J = 0.004 kgm^2 $$, the linear damping coefficient of the wheel is $$ c = 0.015 \frac{Ns}{mrad} $$, the motor constant is $$ K_v = 100 \frac{rad}{sV} $$, and the efficiency is $$ \eta = 0.9 $$. 

With the chosen parameters, the analytical solution $$ x(t) $$ is illustrated for a unit step input ($$ u(\tau)=v_{in}(t)=1 $$) and five randomized initial conditions near zero, showing how each state $$ \omega(t) $$ and $$ \theta(t) $$ respond to such an input. 


<div class="row mt-3">
    <div class="col-12 col-md-7 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/step-comparisons.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Unit step response of the motor simulator states with randomized initial conditions.
</div>

Finally, the poles of the system are analyzed, which dictate the stability and how a response may oscillate and settle over time. THe poles are the eigenvalues of the state matrix A:

$$
A=\begin{bmatrix}\frac{-\eta}{JR_mK_v^2}-\frac{c}{J}&0\\1&0\end{bmatrix}=\begin{bmatrix}-3.7388&0\\1&0\end{bmatrix}
$$

The characteristic equation is $$ \Delta(s) = s(s+3.7388) $$ thus the system has 2 poles, one at 0 and one at -3.7388. 

---

### Step Response

In this section, motor velocity $$ \omega(t) $$ step responses from the motor plant simulator will be normalized and singular value decomposition (SVD) will be used to estimate the nonzero pole $$ a $$. The first order dynamics of the system can be modeled by a pole at $$ a $$ and a DC gain $$ K_{DC} $$, which is estimated from the final value of a response once the system has settled to a stable state. With zero initial conditions, these dynamics can be expressed as

$$
\dot{x}=ax+\frac{K_{DC}}{a}v_{in}
$$

the $$ k $$ step response to which is $$ x(t) = kK_{DC}(1-e^{at}) $$. With simulated data allowing nonzero initial conditions, however, the first-order step response becomes $$ x(t) = kK_{DC}+(x_0-kK_{DC})e^{at} $$. The normalization is thus:

$$
\hat{x}(t)=1-\frac{x(t)-x_0}{kK_{DC}-x_0}=e^{at}
$$

This normalization allows for the estimation of $$ a $$ through the SVD least squares fit of $$ \ln \hat{x}(t) = at $$.

For 50 velocity simulations, the normalized responses are shown. Initial conditions for $$ \omega(0) $$ were randomized in [-6,6] rad/s and $$ \theta(0) $$ in [-0.5,0.5] rad. The step magnitude $$ k $$ was randomly chosen in [1,12] V. The DC gain $$ K_{DC} $$ was estimated using the final 10 percent of each response.

<div class="row mt-3">
    <div class="col-12 col-md-7 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/normalized-responses.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Normalized step responses from 50 simulations of the motor velocity with randomized initial conditions and step inputs.
</div>

These simulations returned an average pole of $$ m_a = -3.50 $$ with standard deviation $$ \sigma_a=1.00 $$ using a fine mesh of $$ \Delta t = 0.001 $$ over a 3 second total time. With the noise and nonlinearities included in the simulator, the pole falls within the expected range of values predicted by the white box approach. Using a more coarse time interval leads to high-variance least squares results, most likely due to the noise present.

---

### Frequency Response

In this section, a sinusoidal input is used to determine the frequency response of the system. An input $$ u(t) = A\sin(\omega t) $$ results in a response of the form $$ x(t) = GA\sin(\omega t + \phi) $$ with a gain $$ G $$ and phase shift $$ \phi $$ which are found using a gradient descent method. Gradient descent methods update the desired parameters by means of predefined loss functions, which are used here to fit sine waves to the motor plant output at different frequencies, and to fit the first order Bode plot. To mirror data sampled from the step response simulations, the sinusoidal input is forced to be positive in the range [0,12] V using $$ u(t) = V_{bias} + A\sin(\omega t) $$ where $$ V_{bias} = 6 $$ and $$ A \in (0,6] $$.

To fit the sinusoidal response, the motor plant output is assumed to take the form $$ y(t) \approx a\sin(\omega t) + b\cos(\omega t) $$ and the model is initialized as $$ \hat{y}(t) = a\sin(\omega t) + b\cos(\omega t) $$. The residual error is $$ e(t) = \hat{y}(t)-y(t) $$. The mean squared error loss function is then defined as:

$$
J(a,b) = \frac{1}{N} \sum_{i=1}^N [e(t_i)^2]^2 = \frac{1}{N} \sum_{i=1}^N [a\sin(\omega t_i) + b\cos(\omega t_i) - y(t_i)]^2
$$

with updates $$ a \leftarrow a - lr \frac{\partial{J}}{\partial{a}} $$ and $$ b \leftarrow b -lr\frac{\partial{J}}{\partial{b}} $$ where $$ lr $$ is the learning rate. 

These updates allow for the amplitude, gain, and phase to be computed as $$ A = \sqrt{a^2+b^2} $$, $$ G = 20\log_{10}(A) $$, and $$ \phi = \tan^{-1}\left(\frac{b}{a}\right) $$ respectively. A similar approach is used to fit the Bode plots from the simulated data. The transfer function for a first order Bode plot assumes the model $$ G(s) = \frac{K}{1+s\tau} $$. Additionally, the following equations are used to simplify the gain magnitude $$ M(\omega) $$ and phase $$ \phi(\omega) $$ in terms of the relevant parameters $$ K $$ and $$ \tau $$:

$$
M(\omega)=20\log_{10}|G(i\omega)|=20\log_{10}K-10\log_{10}(1+\omega^2\tau^2) \quad\quad\quad \phi(\omega)=\angle G(i\omega)=-\tan^{-1} (\omega \tau) \frac{180}{\pi}
$$

The desired parameters $$ \theta $$, in the logarithmic space, are thus the DC gain and the time constant: $$ \theta = \begin{bmatrix}\log K&\log\tau\end{bmatrix}^T $$. The magnitude and phase errors are defined as $$ e_m(\omega)=M_{model}(\omega)-M_{data}(\omega) $$ and $$ e_p(\omega) = \phi_{model}(\omega)-\phi_{data}(\omega) $$ respectively. The loss function is defined as:

$$
J(\theta)=\sum_{i=1}^N [e_m(\omega_i)^2+e_p(\omega_i)^2]
$$

with the vector update $$ \theta \leftarrow \theta - lr \nabla_{\theta}J $$. Learning rates are adjusted each sweep. The gradient descent methods estimated the DC gain $$ K = 1.5 $$, time constant $$ \tau = 0.248 $$ s, and thus a pole located at frequency $$ p = -4.04 $$ rad/s. 

<div class="row mt-3">
    <div class="col-12 col-md-7 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/bode-plot.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The first order Bode plot for the frequency response of the motor plant's velocity. A stable pole is estimated as p = -4.04 rad/s. 
</div>

---

### Black Box Approaches

The following sections involve using black-box techniques to generate predictive models of the motor plant simulator.

#### Radial Basis Function Network

The purpose of the radial basis function network (RBFN) is to approximate the discretized function that represents the electromechanical system:

$$
x_{k+1}=f(x_k,u_k)
$$

To train the RBFN, data from the step and frequency response simulations are loaded into $(x_i, y_i)$ pairs where
$x_i = \begin{bmatrix} x_k & u_k \end{bmatrix}^T$ and $y_i = x_{k+1}$ for each trajectory. The goal is to build the radial basis function matrix for a batch.

$$
\Phi(X)_{n,i}=\phi(x_n)=\exp\!\left(-\epsilon_i \lVert z_n-c_i\rVert^2\right)
$$

where centers $c_i$ are chosen by k-means (for several candidate number of centers $k$). Each data point is assigned to the nearest center, creating clusters where each center is recomputed as the mean of the points in its cluster. This is done iteratively until the shift of centers is below a certain tolerance. $\epsilon_i$ is found by taking the largest eigenvalue of the covariance matrix of the cluster $C_i$. Once the radial basis function matrix is built, SVD is used to find the weights $W$ that minimize $\lVert \Phi W - Y_{tr} \rVert$ where $Y_{tr}$ are the targets (next states). The process is repeated for each $k$ until the one with the lowest root mean squared error is selected for the final fit. The model's capabilities can be visually inspected below, which shows the scope of the training data that was used.

<div class="row mt-3">
    <div class="col-12 col-md-7 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/training-coverage.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    A scatter plot of training data (inputs and states) used on the RBFN (and later, MLP) from step and frequency response simulations. This figure will later be used to test the performance of the black-box techniques against the motor plant simulator. 
</div>

#### Multi-Layered Perceptron

As a feedforward neural network, the multi-layered perceptron (MLP) approach consists of a similar architecture to that of a RBFN, except it typically has much more hidden layers where weighted inputs are passed through nonlinear activation functions. The goal remains the same, to learn a discretized state update model

$$
x_{k+1}=f(x_k,u_k)
$$

where $$ f $$ is a nonlinear mapping approximated by an MLP which is initilized here using the Tensorflow Python library. Training is done in mini-batches with the Adam optimizer and mean squared error as the loss function. Similar to RBFN training, the architecture with the lowest root means squared error is selected and saved as the final fit.

As noted in the training coverage, data regarding step responses form horizontal bands across randomly generated initial conditions. However, most of the training data is dominated by frequency response data which covers the full range of input but only a limited amount of initial conditions. The density of the scatter plot implies strong black-box performance for positive initial conditions. By experimenting with different inputs and initial conditions, the strengths and shortcomings of the training data and RBFN/MLP methods are revealed.

<div class="row mt-3">
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/step-good.jpg"
      class="img-fluid rounded z-depth-1"
    %}
  </div>
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/sine-good.jpg"
      class="img-fluid rounded z-depth-1"
    %}
  </div>
</div>
<div class="row mt-3">
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/step-bad.jpg"
      class="img-fluid rounded z-depth-1"
    %}
  </div>
  <div class="col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/sine-bad.jpg"
      class="img-fluid rounded z-depth-1"
    %}
  </div>
</div>
<div class="caption">
  Top: step (left) and frequency (right) responses with zero initial conditions - both the RBFN and MLP gradually interpolate with less bias as the states are updated. Bottom: step (left) and frequency (right) responses with negative conditions, where the black box methods underperform due to the limitations of the training coverage.
</div>

To improve these models, it is obvious that a wider range of initial conditions must be simulated and sampled. However, under certain conditions, the black-box methods are capable of low-bias approximations of both step and frequency responses.

#### Black Box Conclusions

Nonlinear systems often respond differently to small versus large inputs. Larger inputs lead to faster rise times and higher steady-state velocities, as expected from motor physics. Both the MLP and RBFN capture the general trend of higher step magnitudes, but at a certain point extrapolation becomes necessary due to the limitations of the training set.

<div class="row mt-3">
    <div class="col-12 col-md-7 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/step-comparison.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Comparisons among varying step magnitudes. While the MLP and RBFN trend well, they struggle to interpolate at the low and high extremes of the training set.
</div>

Regarding these two approaches, the RBFN is generally good for interpolation and weaker for extrapolation due to it being a local fit sensitive to training data density. The MLP, however, can generalize better but may smooth way details (such as the noise and nonlinearities included in the simulator). In terms of implementation, the RBFN training has proven to be way easier and practical as k-means and SVD are deterministic and require little to no tuning. On the other hand, the MLP requires the tuning of several hyperparameters (epochs, hidden layers, iterations, etc.) and may perform better as a global function approximator through iterative design. The RBFN performs well for scenarios that fall inside the training coverage whereas the MLP is expected to be more generalizable for unseen states or inputs. 
