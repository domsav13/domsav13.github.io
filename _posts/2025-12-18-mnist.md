---
layout: post
title: Building Deep Neural Networks from Scratch
date: 2025-12-18 11:12:00-0400
description: Training of neural networks on the MNIST dataset
tags: 
categories: 
thumbnail: assets/img/mnist.jpg
related_posts: false
---

### The MNIST Dataset

The Modified National Institute of Standards and Technology (MNIST) dataset is one of the most widely used benchmark datasets in machine learning and pattern recognition. It was designed to provide a standardized, relatively simple testbed for evaluating image classification algorithms, particularly neural networks.

<div class="row mt-3">
    <div class="col-12 col-md-8 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/mnist.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Several samples of handwritten digit images and its label from the MNIST dataset (corochann.com).
</div>

MNIST consists of 70,000 grayscale images of handwritten digits (0-9), each sized as $$ 28 \times 28 $$ pixels. The dataset is split into 60,000 training samples and 10,000 test samples. Every pixel has an intensity value from 0 to 255, representing the brightness of that pixel. A key property of MNIST is that the digits are centered and size-normalized, meaning each image is processed such that the digit is located near the center of the frame and scaled to fit within a bounding box while preserving aspect ratio. This preprocessing enables models to focus on recognizing the underlying digit structured with reduced variability.

MNIST's simplicity makes it ideal for experimentation with neural networks and it has become a "solved dataset," meaning modern deep learning models routinely achieve close to 100% accuracy. In this project, the MNIST dataset provides a consistent and interpretable environment in which to evaluate several fully connected deep neural network architectures.

---

### Neural Network Architectures

Four different neural networks are trained and tested on the MNIST dataset, each with unique architectures. For each neural network, the input layer consists of the collection of all pixels for each digit while the output layer is the predicted digit 0 to 9. Thus, the dimension of the input is $$ 28 \times 28 = 784 $$ and the dimension of the output is 10.

The first architecture (**DNN 784-128-64-10**) represents a classic multilayered perceptron designed for MNIST classification, consisting of an input layer of 784 neurons, two hidden layers of 128 and 64 neurons, and an output layer of 10 neurons. This network mainly serves as a baseline model, balancing complexity with strong performance.

The second network (**DNN 784-256-128-64-10**) extends the baseline into a deeper architecture, consisting of three hidden layers of neuron size 256, 128, and 64. This design increases representational power by widening the first hidden layer and adding depth. The architecture can model more complex nonlinear boundaries and typically yields improved accuracy at the cost of greater risk of overfitting.

The third architecture (**DNN 784-512-10**) increases the width dramatically while remaining shallow with a single hidden layer of 512 neurons. This network demonstrates the universal approximation principle, which states that a single sufficiently wide hidden layer can approximate complex functions. It trains faster and achieves competitive accuracy but may struggle with hierarchical feature extraction.

The fourth architecture (**DNN 784-512-256-128-10**) is the most complex network with three hidden layers of 512, 256, and 128 neurons. This structure allows the network to learn progressively more abstract representations at each layer, encouraging better separation of digit classes, improved generalization, and stronger performance on difficult digits.

---

### Procedures

For this project, each neural network is built from scratch using only the numpy library in Python. The architectures are initialized with weights and biases, and forward and backward passes are hardcoded with the proper activation functions. Then, the training sets are passed through each DNN class.

---

#### Initialization

To keep activations stable during training, the weights $$ W_i $$ for each layer are defined randomly. The bias vectors $$ b_i $$, which allow the decision boundary for each neuron to shift independently of inputs, are initialized as zero vectors. The dimensions of both the weights and biases are adjusted for each of the four proposed neural networks.

---

#### Forward Propagation

Each MNIST image is flattened into a vector with $$ A_0 = x \in \mathbb{R}^{784 \times 1} $$. For each hidden layer, weighted sums are computed for each neuron by linear transformations:

$$
Z_i=W_iA_{i-1}+b_i
$$

The results are then activated via the sigmoid function:

$$
A_i=\sigma(Z_i) \quad\quad\quad \sigma(z)=\frac{1}{1+e^{-z}}
$$

The process is repeated for all hidden layers using the sigmoid function as an activation during training. The output layer uses softmax to estimate the probability that the image corresponds to each digit, so instead of using $$ \sigma(z) $$, the function $$ softmax(z) $$ is used:

$$
softmax(z_i)=\frac{e^{z_i-\max{z}}}{\sum_{j=1}^{10} e^{z_j-\max{z}}}
$$

The forward propagation essentially consists of feature extraction for detecting simple patterns, forming more abstract features via the hidden layers, and finally assigning a probability to the most expected digit.

---

#### Backward Propagation

Backward propagation computes how the network's prediction error should adjust each weight and bias by applying the chain rule from the output layer backward through each hidden layer. The goal is to compute the gradients for all layers $$ l $$:

$$
\frac{\partial L}{\partial W_l} \quad\quad\quad \frac{\partial L}{\partial b_l}
$$

Starting with the output layer for a network with $$ n $$ total layers, the changes in weights and biases are computed as:

$$
\Delta W_n = dZ_nA_{n-1}^T \quad\quad\quad \Delta b_n = dZ_n
$$

From here, the error is propagated backwards via $$ dA_{n-1}=W_n^TdZ_n $$ and the sigmoid activation derivative is applied:

$$
\begin{aligned}
\sigma'(Z_{n-1})&=A_{n-1}(1-A_{n-1}) \\
dZ_{n-1}&=dA_{n-1} \odot \sigma'(Z_{n-1})
\end{aligned}
$$

Then, the gradients are computed as:

$$
\Delta W_{n-1}=dZ_{n-1}A_{n-2}^T \quad\quad\quad \Delta b_{n-1}=dZ_{n-1}
$$

This process is repeated until the input layer. Afterwards, the parameters are updated via stochastic gradient descent. For each parameter matrix and bias vector:

$$
\begin{aligned}
W_l &\leftarrow W_l - \eta\Delta W_l \\
b_l &\leftarrow b_l - \eta\Delta b_l
\end{aligned}
$$

with learning rate $$ \eta = 0.001 $$. This update step moves the parameters in the direction that reduces loss.

---

#### Training

During training, a new neural network class is created with the correct layer sizes, initialized weights and biases, chosen learning rate, and number of epochs. Each neural network is trained using a loop of 10 epochs. An epoch is one full pass through all 60,000 MNIST training images, and 10 epochs serves as a control variable to allow each network to gradually improve. For each epoch, preprocessing, forward propagation, backward propagation, and parameter updates occur.

After each epoch, the accuracy of the trained network is computed. For a network of $$ n $$ layers, the highest-probability class is chosen as:

$$
\hat{y}=\arg \max{A_n}
$$

and the predicted label is compared to the true label in the datset. If $$ \hat{y}=y_{true} $$, the prediction is counted as correct. After each epoch the classification accuracy is computed and recorded as the number of correct predictions divided by the total number of test images.

---

### Results

While all the models typically converge to the same value after 10 epochs, the simplest network performed best, most likely due to MNIST being a relatively simple dataset. Additionally, the moderate number of parameters most likely prevented overfitting as opposed to the other more underperforming models. Notably, the largest and most parameter-heavy model shows that more layers and more parameters does not result in more accuracy. The overall results demonstrate that for basic networks trained with sigmoid activations and no advanced optimization techniques, the best performing models are shallow or moderately deep with balanced layers. Increasing the network size beyond this point introduces complexity without providing additional power for the MNIST digit classification task.

<div class="row mt-3">
    <div class="col-12 col-md-8 mx-auto mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/learning-curves.jpg" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Learning curves for each DNN architecture during the 10-epoch training period.
</div>

The key takeaways show that depth does not guarantee better performance, especially with sigmoid activation. The wider shallow networks often outperform deeper ones when training conditions are limited. The MNIST task is simple enough that excessive depth leads to longer training times, slower convergence, and no meaningful gains in accuracy, and the DNN model 784-128-64-10 finds the best balance between depth, width, and propagation.

---

#### Confusion Matrices

The confusion matrix is an informative tool for evaluating classification models. Unlike overall accuracy, the confusion matrix reveals class-specific performance, patterns in model mistakes, band biases and insights for model improvement. More specifically, it shows how often each digit is correctly classified (on the main diagonal) versus misclassified (on the off-diagonal), showing which digits are hardest to predict for each model.

<div class="row mt-3 g-3 justify-content-center">
  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/cm1.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>

  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/cm2.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>

  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/cm3.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>

  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/cm4.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>
</div>

The models all follow similar trends with classification, with subtle differences. The digits 0, 1, 6, 7, 8, and 9 are predicted with very high accuracy, as shown on the main diagonal. Confusion patterns increase for digit 5 which is often confused with 3 and 8, and digit 9 which is occasionally confused with 4 and 7. For more underperforming models, there are heavier misclassifications for 2 and 7 as well as 8, 9, and 3. Specific examples are examined next.

---

#### Case Studies

Certain digits are inherently easier for neural networks because of their high intra-class consistency and strong distinguishing geometric features. Primarily, the digits 2 and 0 fall into this category.

<div class="row mt-3 justify-content-center">
  <div class="col-12 col-md-5 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/mnist-good-2.jpg"
      class="img-fluid rounded z-depth-1 w-100"
    %}
  </div>

  <div class="col-12 col-md-5 mt-3 mt-md-0">
    {% include figure.liquid
      loading="eager"
      path="assets/img/mnist-good-0.jpg"
      class="img-fluid rounded z-depth-1 w-100"
    %}
  </div>
</div>
<div class="caption">
    Randomly sampled MNIST test digits 2 and 0 that are correctly classified by all four trained networks.
</div>

These digits have shapes that rarely overlap with other categories. The digit 0 has a unique closed loop shape, no intersections or sharp corners, and very different from other digits which may have additional internal structures. The digit 2 has a trademark upper curve, diagonal tail, and does not resemble other digits except occasional confusion with a poorly written 7.

On the other hand, the digit 5 is one of the most commonly misclassified digits. It exhibits ambiguous geometry with two major issues: a strong top horizontal bar (more characteristic of digits like 0 or 8) and a smooth bottom curl (closer to patterns found in 6, 8, or 0). These shape deviations create ambiguity in the early hidden layers, which leads to overfitting and sensitivity to rare distortions.

<div class="row mt-3 justify-content-center">
  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/mnist-bad-5.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>
  <div class="col-12 col-md-5">
    {% include figure.liquid path="assets/img/mnist-good-5.jpg"
      class="img-fluid rounded z-depth-1 w-100" %}
  </div>
</div>
<div class="caption">
    Randomly sampled MNIST test digits processed by the trained models for which 2 incorrectly predict 0 and 8 instead of 5 (left) and all correcly predict 5 (right).
</div>

The last comparison presents two more cases highlighting how the trained networks respond to visual ambiguity in the handwriting. In the left image, all networks incorrectly predict 9 more strongly than 8, most likely due to a more dominant upper loop (resembling the circular top of 9) and a faint and uneven lower loop. This case shows that when an image deviates from the canonical two-loop 8 (as shown in the right image), the learned features consistently bias the networks towards 9. In the middle image, a model incorrectly predicts 8 in the case of 6, as the digit has a strong downward stroke that curves into a lower loop, which may be interpreted as the lower loop of 8 in certain ambiguous cases.

<div class="row mt-3 g-3">
  <div class="col-12 col-md-4">
    {% include figure.liquid
      loading="eager"
      path="assets/img/mnist-bad-8.jpg"
      class="img-fluid rounded z-depth-1 w-100"
    %}
  </div>

  <div class="col-12 col-md-4">
    {% include figure.liquid
      loading="eager"
      path="assets/img/mnist-bad-6.jpg"
      class="img-fluid rounded z-depth-1 w-100"
    %}
  </div>

  <div class="col-12 col-md-4">
    {% include figure.liquid
      loading="eager"
      path="assets/img/mnist-good-8.jpg"
      class="img-fluid rounded z-depth-1 w-100"
    %}
  </div>
</div>
<div class="caption">
  Randomly sampled MNIST test digits processed by the trained models for which all predict the digit 9 instead of 8 (left), all but one correctly identify 6 with one predicting 8 (middle), and all correctly identify 8 (right).
</div>

Across all architectures, performances on the MNIST dataset were consistently strong, with final accuracies ranging from roughly 89 to 92 percent. Overall, the combined results learn the MNIST task well, illustrate the influence of model depth and parameter count on robustness, and the confusion patterns emphasize the inherent variability in handwritten digits rather than failure of the learning process.
