<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://domsav13.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://domsav13.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-11T02:48:57+00:00</updated><id>https://domsav13.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Building deep neural networks from scratch</title><link href="https://domsav13.github.io/blog/2025/mnist/" rel="alternate" type="text/html" title="Building deep neural networks from scratch"/><published>2025-12-18T15:12:00+00:00</published><updated>2025-12-18T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/mnist</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/mnist/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Training of neural networks on the MNIST dataset]]></summary></entry><entry><title type="html">Controls for a 2D rocket</title><link href="https://domsav13.github.io/blog/2025/2drocket/" rel="alternate" type="text/html" title="Controls for a 2D rocket"/><published>2025-12-10T15:12:00+00:00</published><updated>2025-12-10T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/2drocket</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/2drocket/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[LQG and model predictive control of a rocket for hovering, landing, and waypoint flight]]></summary></entry><entry><title type="html">Prosthetic leg</title><link href="https://domsav13.github.io/blog/2025/prosthesis/" rel="alternate" type="text/html" title="Prosthetic leg"/><published>2025-12-08T15:12:00+00:00</published><updated>2025-12-08T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/prosthesis</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/prosthesis/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Requirement analysis and design of a knee prosthesis]]></summary></entry><entry><title type="html">Nonlinear control of a DC motor</title><link href="https://domsav13.github.io/blog/2025/nonlinear-controls/" rel="alternate" type="text/html" title="Nonlinear control of a DC motor"/><published>2025-11-25T15:12:00+00:00</published><updated>2025-11-25T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/nonlinear-controls</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/nonlinear-controls/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Control system design using Lyapunov stability and sliding manifolds]]></summary></entry><entry><title type="html">Basic and optimal control of a double pendulum</title><link href="https://domsav13.github.io/blog/2025/pendulum/" rel="alternate" type="text/html" title="Basic and optimal control of a double pendulum"/><published>2025-10-14T15:12:00+00:00</published><updated>2025-10-14T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/pendulum</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/pendulum/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Control system design using pole placement, linear quadratic regulator, and Kalman filter]]></summary></entry><entry><title type="html">System identification of a DC motor</title><link href="https://domsav13.github.io/blog/2025/sys-id/" rel="alternate" type="text/html" title="System identification of a DC motor"/><published>2025-09-10T15:12:00+00:00</published><updated>2025-09-10T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/sys-id</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/sys-id/"><![CDATA[<p>This project investigates system identification of a DC motor simulator using both analytical and data-driven methods. A white-box state-space model was first derived to establish the expected dynamics, step and frequency responses were measured, and then black-box methods were trained on the data to model and predict the behavior of the full electromechanical system.</p> <hr/> <h3 id="white-box-approach">White Box Approach</h3> <p>An electric DC motor is connected to a wheel with linear damping \(c\) and inertia \(J\). The motor generates a torque \(\tau\) with some angular velocity \(\omega\) which is related to the back emf \(v_{emf}\) by the motor constant \(K_v\). The motor converts electrical power to mechanical power with efficiency \(\eta\), with the equations:</p> \[\begin{aligned} \omega = K_v v_{emf} \quad\quad\quad \eta = \frac{P_{out}}{P_{in}} \end{aligned}\] <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wing-480.webp 480w,/assets/img/wing-800.webp 800w,/assets/img/wing-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wing.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> A DC motor connected to a wheel, the overarching schematic for this system identification analysis. </div> <p>Using the efficiency equation with mechanical and electrical power, the relation between current \(i_m\) and torque \(\tau\) is given by:</p> \[\begin{aligned} P_{out}&amp;=\eta P_{in}\\ \tau \omega &amp;= \eta i_m v_{emf}\\ \tau K_v v_{emf} &amp;= \eta i_m v_{emf}\\ \frac{K_v}{\eta} \tau &amp;= i_m \end{aligned}\] <p>Applying Newton’s law of rotation to the system, the mechanical equation of motion is \(J \dot{\omega} = \tau - c \omega\).</p> <p>Applying Kirchoff’s current and voltage laws with the current \(i_m\), the electrical equation of motion is \(v_{in} = v_{emf} + R_m i_m\).</p> <p>Combining both equations of motion with the given relations:</p> \[\begin{aligned} J\dot{\omega}&amp;=\frac{\eta}{K_v}i_m-c\omega\\ J\dot{\omega}&amp;=\frac{\eta}{R_mK_v}(v-v_{emf})-c\omega\\ J\dot{\omega}&amp;=\frac{\eta}{R_mK_v}(v_{in}-\frac{\omega}{K_v})-c\omega\\ J\dot{\omega}&amp;=\frac{\eta}{R_mK_v}v_{in}-(c+\frac{\eta}{R_mK_v^2})\omega \end{aligned}\] <p>In state-space form, the system is represented by:</p> \[\dot{x}=\begin{bmatrix}\dot{\omega}\\\dot{\theta}\end{bmatrix}=\begin{bmatrix}\frac{-\eta}{JR_mK_v^2}-\frac{c}{J}&amp;0\\1&amp;0\end{bmatrix}\begin{bmatrix}\omega\\\theta\end{bmatrix}+\begin{bmatrix}\frac{\eta}{JR_mK_v}\\0\end{bmatrix}v_{in}=Ax+Bu\] <p>Using these matrices, the analytical solution \(x(t)\) given an input \(u(t)\) is formulated as:</p> \[x(t)=e^{At}x_0+\int_0^t e^{A(t-\tau)}Bu(\tau)d\tau\] <p>For the next analyses, parameters are chosen. The resistance of the motor coil is \(R_m = 2 \Omega\), the moment of inertia of the wheel is \(J = 0.004 kgm^2\), the linear damping coefficient of the wheel is \(c = 0.015 \frac{Ns}{mrad}\), the motor constant is \(K_v = 100 \frac{rad}{sV}\), and the efficiency is \(\eta = 0.9\).</p> <p>With the chosen parameters, the analytical solution \(x(t)\) is illustrated for a unit step input (\(u(\tau)=v_{in}(t)=1\)) and five randomized initial conditions near zero, showing how each state \(\omega(t)\) and \(\theta(t)\) respond to such an input.</p> <p>img</p> <p>Finally, the poles of the system are analyzed, which dictate the stability and how a response may oscillate and settle over time. THe poles are the eigenvalues of the state matrix A:</p> \[A=\begin{bmatrix}\frac{-\eta}{JR_mK_v^2}-\frac{c}{J}&amp;0\\1&amp;0\end{bmatrix}=\begin{bmatrix}-3.7388&amp;0\\1&amp;0\end{bmatrix}\] <p>The characteristic equation is \(\Delta(s) = s(s+3.7388)\) thus the system has 2 poles, one at 0 and one at -3.7388.</p> <hr/> <h3 id="step-response">Step Response</h3> <p>In this section, motor velocity \(\omega(t)\) step responses from the motor plant simulator will be normalized and singular value decomposition (SVD) will be used to estimate the nonzero pole \(a\). The first order dynamics of the system can be modeled by a pole at \(a\) and a DC gain \(K_{DC}\), which is estimated from the final value of a response once the system has settled to a stable state. With zero initial conditions, these dynamics can be expressed as</p> \[\dot{x}=ax+\frac{K_{DC}}{a}v_{in}\] <p>the \(k\) step response to which is \(x(t) = kK_{DC}(1-e^{at})\). With simulated data allowing nonzero initial conditions, however, the first-order step response becomes \(x(t) = kK_{DC}+(x_0-kK_{DC})e^{at}\). The normalization is thus:</p> \[\hat{x}(t)=1-\frac{x(t)-x_0}{kK_{DC}-x_0}=e^{at}\] <p>This normalization allows for the estimation of \(a\) through the SVD least squares fit of \(\ln \hat{x}(t) = at\).</p> <p>For 50 velocity simulations, the normalized responses are shown. Initial conditions for \(\omega(0)\) were randomized in [-6,6] rad/s and \(\theta(0)\) in [-0.5,0.5] rad. The step magnitude \(k\) was randomly chosen in [1,12] V. The DC gain \(K_{DC}\) was estimated using the final 10 percent of each response.</p> <p>img</p> <p>These simulations returned an average pole of \(m_a = -3.50\) with standard deviation \(\sigma_a=1.00\) using a fine mesh of \(\Delta t = 0.001\) over a 3 second total time. With the noise and nonlinearities included in the simulator, the pole falls within the expected range of values predicted by the white box approach. Using a more coarse time interval leads to high-variance least squares results, most likely due to the noise present.</p> <hr/> <h3 id="frequency-response">Frequency Response</h3> <p>In this section, a sinusoidal input is used to determine the frequency response of the system. An input \(u(t) = A\sin(\omega t)\) results in a response of the form \(x(t) = GA\sin(\omega t + \phi)\) with a gain \(G\) and phase shift \(\phi\) which are found using a gradient descent method. Gradient descent methods update the desired parameters by means of predefined loss functions, which are used here to fit sine waves to the motor plant output at different frequencies, and to fit the first order Bode plot. To mirror data sampled from the step response simulations, the sinusoidal input is forced to be positive in the range [0,12] V using \(u(t) = V_{bias} + A\sin(\omega t)\) where \(V_{bias} = 6\) and \(A \in (0,6]\).</p> <p>To fit the sinusoidal response, the motor plant output is assumed to take the form \(y(t) \approx a\sin(\omega t) + b\cos(\omega t)\) and the model is initialized as \(\hat{y}(t) = a\sin(\omega t) + b\cos(\omega t)\). The residual error is \(e(t) = \hat{y}(t)-y(t)\). The mean squared error loss function is then defined as:</p> \[J(a,b) = \frac{1}{N} \sum_{i=1}^N [e(t_i)^2]^2 = \frac{1}{N} \sum_{i=1}^N [a\sin(\omega t_i) + b\cos(\omega t_i) - y(t_i)]^2\] <p>with updates \(a \leftarrow a - lr \frac{\partial{J}}{\partial{a}}\) and \(b \leftarrow b -lr\frac{\partial{J}}{\partial{b}}\) where \(lr\) is the learning rate.</p> <p>These updates allow for the amplitude, gain, and phase to be computed as \(A = \sqrt{a^2+b^2}\), \(G = 20\log_{10}(A)\), and \(\phi = \tan^{-1}\left(\frac{b}{a}\right)\) respectively. A similar approach is used to fit the Bode plots from the simulated data. The transfer function for a first order Bode plot assumes the model \(G(s) = \frac{K}{1+s\tau}\). Additionally, the following equations are used to simplify the gain magnitude \(M(\omega)\) and phase \(\phi(\omega)\) in terms of the relevant parameters \(K\) and \(\tau\):</p> \[M(\omega)=20\log_{10}|G(i\omega)|=20\log_{10}K-10\log_{10}(1+\omega^2\tau^2) \quad\quad\quad \phi(\omega)=\angle G(i\omega)=-\tan^{-1} (\omega \tau) \frac{180}{\pi}\] <p>The desired parameters \(\theta\), in the logarithmic space, are thus the DC gain and the time constant: \(\theta = \begin{bmatrix}\log K&amp;\log\tau\end{bmatrix}^T\). The magnitude and phase errors are defined as \(e_m(\omega)=M_{model}(\omega)-M_{data}(\omega)\) and \(e_p(\omega) = \phi_{model}(\omega)-\phi_{data}(\omega)\) respectively. The loss function is defined as:</p> \[J(\theta)=\sum_{i=1}^N [e_m(\omega_i)^2+e_p(\omega_i)^2]\] <p>with the vector update \(\theta \leftarrow \theta - lr \nabla_{\theta}J. Learning rates are adjusted each sweep. The gradient descent methods estimated the DC gain\) K = 1.5 \(, time constant\) \tau = 0.248 \(s, and thus a pole located at frequency\) p = -4.04 $$ rad/s.</p> <p>img</p> <hr/> <h3 id="black-box-approaches">Black Box Approaches</h3> <p>The following sections involve using black-box techniques to generate predictive models of the motor plant simulator.</p> <h4 id="radial-basis-function-network">Radial Basis Function Network</h4> <p>The purpose of the radial basis function network (RBFN) is to approximate the discretized function that represents the electromechanical system:</p> \[x_{k+1}=f(x_k,u_k)\] <p>To train the RBFN, data from the step and frequency response simulations are loaded into \((x_i,y_i)\) pairs where \(x_i = \begin{bmatrix}x_k&amp;u_k\end{bmatrix}^T\) and \(y_i=x_{k+1}\) for each trajectory. The goal is to biuld the radial basis function matrix for a batch</p> \[\Phi(X)_{n,i}=\phi(x_n)=\exp(-\epsilon_i ||z_n-c_i||^2)\] <table> <tbody> <tr> <td>where centers \(c_i\) are chosen by k-means (for several candidate number of centers \(k\)). Each data point is assigned to the nearest center, creating clusters where each center is recomputed as the mean of the points in its cluster. This is done iteratively until the shift of centers is below a certain tolerance. \(\epsilon_i\) is found by taking the largest eigenvalue of the covariance matrix of the cluster \(C_i\). Once the radial basis function matrix is built, SVD is used to find the weights \(W\) that minimize $$</td> <td> </td> <td>\Phi W - Y_{tr}</td> <td> </td> <td>\(where\) Y_{tr} \(are the targets (next states). The process is repeated for each\) k $$ until the one with the lowest root means squared error is selected for the final fit. The model’s capabilities can be visually inspected below, which shows the scope of the training data that was used.</td> </tr> </tbody> </table> <p>img</p> <h4 id="multi-layered-perceptron">Multi-Layered Perceptron</h4> <p>As a feedforward neural network, the multi-layered perceptron (MLP) approach consists of a similar architecture to that of a RBFN, except it typically has much more hidden layers where weighted inputs are passed through nonlinear activation functions. The goal remains the same, to learn a discretized state update model</p> \[x_{k+1}=f(x_k,u_k)\] <p>where \(f\) is a nonlinear mapping approximated by an MLP which is initilized here using the Tensorflow Python library. Training is done in mini-batches with the Adam optimizer and mean squared error as the loss function. Similar to RBFN training, the architecture with the lowest root means squared error is selected and saved as the final fit.</p> <p>As noted in the training coverage, data regarding step responses form horizontal bands across randomly generated initial conditions. However, most of the training data is dominated by frequency response data which covers the full range of input but only a limited amount of initial conditions. The density of the scatter plot implies strong black-box performance for positive initial conditions. By experimenting with different inputs and initial conditions, the strengths and shortcomings of the training data and RBFN/MLP methods are revealed.</p> <p>2 imgs good 2 imgs bad</p> <p>To improve these models, it is obvious that a wider range of initial conditions must be simulated and sampled. However, under certain conditions, the black-box methods are capable of low-bias approximations of both step and frequency responses.</p> <h4 id="black-box-conclusions">Black Box Conclusions</h4> <p>Nonlinear systems often respond differently to small versus large inputs. Larger inputs lead to faster rise times and higher steady-state velocities, as expected from motor physics. Both the MLP and RBFN capture the general trend of higher step magnitudes, but at a certain point extrapolation becomes necessary due to the limitations of the training set.</p> <p>2 plots step</p> <p>Regarding these two approaches, the RBFN is generally good for interpolation and weaker for extrapolation due to it being a local fit sensitive to training data density. The MLP, however, can generalize better but may smooth way details (such as the noise and nonlinearities included in the simulator). In terms of implementation, the RBFN training has proven to be way easier and practical as k-means and SVD are deterministic and require little to no tuning. On the other hand, the MLP requires the tuning of several hyperparameters (epochs, hidden layers, iterations, etc.) and may perform better as a global function approximator through iterative design. The RBFN performs well for scenarios that fall inside the training coverage whereas the MLP is expected to be more generalizable for unseen states or inputs.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[White- and black-box approaches to motor control]]></summary></entry><entry><title type="html">MAE Senior Capstone</title><link href="https://domsav13.github.io/blog/2025/capstone/" rel="alternate" type="text/html" title="MAE Senior Capstone"/><published>2025-05-10T15:12:00+00:00</published><updated>2025-05-10T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/capstone</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/capstone/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Design and build of a maritime spotlight]]></summary></entry><entry><title type="html">IoT Christmas Tree</title><link href="https://domsav13.github.io/blog/2025/xmas-tree/" rel="alternate" type="text/html" title="IoT Christmas Tree"/><published>2025-05-10T15:12:00+00:00</published><updated>2025-05-10T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/xmas-tree</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/xmas-tree/"><![CDATA[<p>This project presents the development of an IoT-enabled Christmas tree featuring dynamic lighting and motion responsive to sensor data and user control. A Raspberry Pi manages a WS2811 RGB LED strip, BH1750 ambient light sensor, motion sensors, and servo-driven animatronic eyes with custom 3D-printed housing. Python libraries including rpi_ws281x and Flask were used for real-time LED control and web-based customization. A six-layer IoT model guided the system architecture, combining perception, data processing, actuation, networking, and user interfacing. 3D spatial mapping of the LEDs enabled complex, geometry-based animations, while real-time ambient brightness was visualized through a web dashboard.</p> <hr/> <h3 id="methods">Methods</h3> <p>Regarding setup and LED handling, a 2-foot Christmas tree was purchased along with a strip of 50 WS2811 programmable LEDs. Additionally, an independent 5 V power supply was purchased with a DC adapter that wires directly to the LED strip via WAGO connectors. The LEDs were positioned uniformly throughout the tree for the best visual experience; due to the circuitry of the LED strip, the LEDs light up in the order they are wired when they are sequentially programmed. To achieve a broader spectrum of animations (especially those based on geometry), 3D coordinates were collected for each LED. By using a 360-degree protractor and a tape measure, polar cooridnates were collected and then converted to Cartesian coordinates for use in generating complex light patterns.</p> <p>The I2C BH1750 light sensor was chosen to detect changes in ambient brightness in tandem with the lighting setup. Regardless of LED effects, this sensor dims or amplifies brightness based on ambient light. The ambient light (and subsequent LED brightness) is published on the user interface so that peak usage times can be estimated for a continuously lit tree and energy consumption can be decreased where appropriate.</p> <div class="row mt-3"> <div class="col-12"> <div class="row justify-content-center"> <div class="col-12 col-md-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/xmas-tree-480.webp 480w,/assets/img/xmas-tree-800.webp 800w,/assets/img/xmas-tree-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/xmas-tree.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-12 col-md-5"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bh1750-480.webp 480w,/assets/img/bh1750-800.webp 800w,/assets/img/bh1750-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/bh1750.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> </div> <div class="caption"> Once the tree was set up, polar coordinates were utilized to generate 50 3D coordinates (X,Y,Z) of each LED for geometry-based animations. The microcomputer also used ambient light readings delivered by the BH1750 sensor to control brightness of the LEDs. </div> <hr/> <h3 id="results">Results</h3> <p>For animating the lights, several mathematical- and time-based effects were brainstormed including fireworks, twisters, heartbeats, contagious (proximity-based) effects, and random planes or rotations. Music synchronization was also explored, but remains an item under further work as automating, synchronizing, and mapping music to LED effects requires advanced signal processing methods. The code can be found at <a href="https://github.com/domsav13/iotxmastree">this GitHub repo</a>.</p> <div class="row mt-3 g-3 align-items-start"> <div class="col-12 col-md-4"> <video autoplay="" loop="" muted="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/contagious.mp4" type="video/mp4"/> </video> </div> <div class="col-12 col-md-4"> <video autoplay="" loop="" muted="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/rainbow.mp4" type="video/mp4"/> </video> </div> <div class="col-12 col-md-4"> <video controls="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/mariah-carey.mp4" type="video/mp4"/> </video> </div> </div> <div class="caption"> Simple light animations (proximity-based contagious and rainbow spiral effects). Mariah Carey's "All I Want for Christmas Is You" was also synchronized to the lights, although this was done manually instead of automated. </div> <hr/> <p>In Fall 2025, the project was expanded to a 500 LED, 7-foot Chrismas tree for the MAE department at GW. Instead of manually recording coordinates for each LED, the tree was modeled as a cone with an ascending helix representative of the LEDs spiraling throughout the tree. Using mathematical models of cones and spirals and taking basic measurements such as the upper and lower tree radii and number of turns, the 3D coordinates were parameterized for each LED index.</p> <div class="row mt-3"> <div class="col-12 col-md-6 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/500-leds-480.webp 480w,/assets/img/500-leds-800.webp 800w,/assets/img/500-leds-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/500-leds.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Wiring setup for the 500 LEDs (10 strips of 50 LEDs). The wiring required a rethinking of power distribution, as powering the LEDs in series results in diminished brightness towards the end of the strip. A solution was to inject power at both the beginning and end of the total length, which resulted in a better balance of LED brightness. </div> <p>In regards to the animations, the jump from 50 to 500 LEDs significantly increases resolution, which supports more complex effects. The full work can be found at <a href="https://github.com/domsav13/maexmastree">this GitHub repo</a>.</p> <div class="row mt-3 g-3 align-items-start"> <div class="col-12 col-md-4"> <video autoplay="" loop="" muted="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/500-contagious.mp4" type="video/mp4"/> </video> </div> <div class="col-12 col-md-4"> <video autoplay="" loop="" muted="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/up-down.mp4" type="video/mp4"/> </video> </div> <div class="col-12 col-md-4"> <video autoplay="" loop="" muted="" playsinline="" preload="metadata" class="img-fluid rounded z-depth-1 w-100"> <source src="/assets/img/snakes.mp4" type="video/mp4"/> </video> </div> <div class="caption"> Similar LED effects on the 500 LED, 7-foot Christmas tree (contagious, vertical, and snake effects). The tree is modeled as a cone with an ascending spiral representing the mapping of each LED. </div> </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Coordinate- and sensor-based Christmas tree lighting]]></summary></entry><entry><title type="html">Object Tracking and Identification</title><link href="https://domsav13.github.io/blog/2025/opencv/" rel="alternate" type="text/html" title="Object Tracking and Identification"/><published>2025-03-10T15:12:00+00:00</published><updated>2025-03-10T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2025/opencv</id><content type="html" xml:base="https://domsav13.github.io/blog/2025/opencv/"><![CDATA[<p>This project explores computer vision-based object detection and facial recognition using a Raspberry Pi 4B and PiCamera V2.1 to create a compact, real-time security/detection system. The detection model captures video frames, processes them in real time, and sends notifications when a target is detected. OpenCV, a widely used computer vision library, and the COCO dataset were used to develop and train the model. The object detection performed reliably with adjustable confidence thresholds, but facial recognition struggled with prolonged use due to hardware limitations.</p> <hr/> <h3 id="background">Background</h3> <p>Using a three-layer IoT model incorporating the perception, network, and application layers, the final product seeks to generate email notifications based on visual detection methods. The work is inspired by security and privacy computer vision-based systems (Ring, Apple Face ID, etc.).</p> <p>OpenCV is a free computer vision and machine learning library built to advance machine perception in commercial products, featuring thousands of optimized algorithms that can be used to detect and recognize faces, identify objects, classify gestures, and more. The fundamentals of object detection in OpenCV involve two main tasks: classification and localization. Most methods work to determine the exact location of an object within an image or video feed by using bounding boxes or segmentation. The promises of OpenCV rely fully on the models that are provided, which is why the Common Objects in Context (COCO) dataset is essential for the success of the detection performance.</p> <p>COCO is a large-scale object detection dataset that features recognition of over 200,000 labeled images among 1.5 million object instances. The datasets are open source and available for download, and are designed to train and evaluate detection models with rich contextual data and precise localization via “segmentation masks,” a specific portion of an image that is isolated from the rest. This project will use OpenCV algorithms to train these labeled images so that a Raspberry Pi and PiCamera can detect objects and alert the user in real time.</p> <hr/> <h3 id="methods-and-implementation">Methods and implementation</h3> <p>The requirements for this project include a Raspberry Pi 4 (flashed with the Buster version of the Debian OS) and PiCamera V2.1. The camera interfaces with the microcomputer with a ribbon connector.</p> <div class="row mt-3"> <div class="col-12 col-md-5 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/eyepi-workflow-480.webp 480w,/assets/img/eyepi-workflow-800.webp 800w,/assets/img/eyepi-workflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/eyepi-workflow.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Software workflow for the email notification and object detection system. </div> <p>The primary goal of the code is to detect a specific object, capture an image when the object is detected, and send an email with the image attached. The pre-trained COCO dataset incorporates the labeling of 91 common everyday objects. The object detection model is configured using these COCO files to define class labels and specify weights, which is later loaded by the OpenCV deep neural network module. A video capture continuously reads frames and passes them through a function that checks whether the detected object matches the target object specified by the user. If so, it draws a bounding box and a label around the object on the image, saves it, and sends it via email. This function is central to the object detection process, as it uses a confidence and non-maximum suppression (NMS) threshold. The confidence threshold filters out low-confidence detections and the NMS thresholds reduces overlapping boxes to avoid duplicate detections. The email is sent using SMTP, EmailMessage, and a Gmail account with app passwords enabled to include a subject and body message indicating that the object was detected, attaching the saved image as well.</p> <p>All code is presented at <a href="https://github.com/domsav13/eyepi">this GitHub repo</a>.</p> <hr/> <h3 id="results">Results</h3> <p>Overall, the methods satisfied basic requirements but were limited due to hardware constraints. The limited computing power of the Raspberry Pi and low frame rates in the video feed limited the performance of the software, however the object detection model met expectations.</p> <div class="row mt-3"> <div class="col-12"> <div class="row justify-content-center"> <div class="col-12 col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/detection-left-480.webp 480w,/assets/img/detection-left-800.webp 800w,/assets/img/detection-left-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/detection-left.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-12 col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/detection-right-480.webp 480w,/assets/img/detection-right-800.webp 800w,/assets/img/detection-right-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/detection-right.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> </div> <div class="caption"> Exclusive search within the video feed, filtering out specified objects. Bounding boxes and confidence levels are shown around each object. Left: objects=[""]; Right: objects=["CUP"]. </div> <p>There are a few hyperparameters that the user can modify. First, using an exclusive search feature of OpenCV, the user may specify which object or group of objects the detection model should look for. In other words, the user can filter out undesired objects from the detection model if only searching for instances of objects of interest. Additionally, the user may choose the confidence threshold for the detection. While objects such as cups, chairs, or people are easily detectable, there are fewer common objects that require lower confidence thresholds. A high confidence level is typically above 0.7 whereas a poor confidence is below 0.4.</p> <div class="row mt-3"> <div class="col-12"> <div class="row justify-content-center"> <div class="col-12 col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/broccoli-detected-480.webp 480w,/assets/img/broccoli-detected-800.webp 800w,/assets/img/broccoli-detected-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/broccoli-detected.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-12 col-md-6"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/chair-detected-480.webp 480w,/assets/img/chair-detected-800.webp 800w,/assets/img/chair-detected-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/chair-detected.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </div> </div> <div class="caption"> The COCO dataset used here contains 91 everyday objects, but not all are detected with high confidence by the OpenCV model. The difficulty is reflected in the confidence metric that is displayed with each bounding box. </div> <p>Lastly, the implementation of a personal security system that detects people and sends email alerts to the home user (inspired by Ring) works exceptionally well due to the high confidence of “person” in the detection model and simple, rapid communication via WiFi. Not only does the email notification include a snapshot of the detected object, it also provides a timestamp and a description of which object was detected.</p> <div class="row mt-3"> <div class="col-12 col-md-5 mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/eyepi-email-480.webp 480w,/assets/img/eyepi-email-800.webp 800w,/assets/img/eyepi-email-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/eyepi-email.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Sample email notification with detection timestamp and "person" as the target object, sent by a third-party email account. </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Pocket-sized computer vision (OpenCV)]]></summary></entry><entry><title type="html">Mini Foosball Table</title><link href="https://domsav13.github.io/blog/2024/foosball/" rel="alternate" type="text/html" title="Mini Foosball Table"/><published>2024-11-25T15:12:00+00:00</published><updated>2024-11-25T15:12:00+00:00</updated><id>https://domsav13.github.io/blog/2024/foosball</id><content type="html" xml:base="https://domsav13.github.io/blog/2024/foosball/"><![CDATA[<p>The goals of this manufacturing project were to define product specifications using CAD software, outline component requirements by developing manufacturing process plans, gain experience with simple machine shop machines (lathe, mill, drill press, etc.), manage internal and external resources, and build and test the final design.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/foosball-1-480.webp 480w,/assets/img/foosball-1-800.webp 800w,/assets/img/foosball-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/foosball-1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/foosball-2-480.webp 480w,/assets/img/foosball-2-800.webp 800w,/assets/img/foosball-2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/foosball-2.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Final product of the mini foosball table. </div> <p>The project also highlights the important differences between design for manufacturability (DFM) and design for assembly (DFA). DFM focuses on making individual parts easy and cost-effective to manufacture while DFA focuses on minimizing the time, cost, and errors involved in assembling those parts into a finished product. There are several tradeoffs that were observed between DFM and DFA, trying to bring a CAD model to life, and through understanding the practical capabilities of the available machines.</p> <hr/> <h3 id="design">Design</h3> <p>Each component was designed and modeled in SOLIDWORKS to visualize a baseline assembly. This version featured 10 player models and had dimensions 12.5 x 19.75 inches.</p> <div class="row mt-3"> <div class="col-12"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/foosball-assembly-480.webp 480w,/assets/img/foosball-assembly-800.webp 800w,/assets/img/foosball-assembly-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/foosball-assembly.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The initial CAD assembly for the mini foosball table, showing the original key components: base, goal walls, side walls, player, and rod. </div> <p>This assembly was helpful in streamlining the manufacturing process plans for the key components, but it is not representative of the final product for several reasons. During the manufacturing phase, back walls were designed and manufactured to create a space to collect the ball once a goal has been scored; bushings were selected to allow near-frictionless movement of the rods through the through holes in the side walls; player heads were 3D printed in two different colors to distinguish between teams; handles were manufactured and attached to the rods for comfortable control of players; and bumpers were added to rods with 2 players to prevent unwanted contact with the side walls.</p> <hr/> <h3 id="manufacturing-and-assembly">Manufacturing and assembly</h3> <p>A wide variety of machines were used in the manufacturing processes, including the lathe, mill, band and drop saws, and drill press. The saws were primarily used to make preliminary cuts in the raw material for each component. The drill press was used to make through holes in the side walls and the player bases. The lathe was used to turn down the players (machined using stock Aluminum) into equal sizes and was then programmed to do a revolve cut to achieve a geometry similar to a standard foosball player model. A similar approach was used to turn down the ends of the handles to make them hemispherical in shape. The lathe was further used to drill center holes in the tops of the players (for assembly with the 3D printed heads) and the handles (for assembly with the rods).</p> <swiper-container class="media-swiper" keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/player-sketch-480.webp 480w,/assets/img/player-sketch-800.webp 800w,/assets/img/player-sketch-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/player-sketch.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/player-lathe-cutting-path-480.webp 480w,/assets/img/player-lathe-cutting-path-800.webp 800w,/assets/img/player-lathe-cutting-path-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/player-lathe-cutting-path.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <video src="/assets/img/lathe_player.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lathe-drilling-480.webp 480w,/assets/img/lathe-drilling-800.webp 800w,/assets/img/lathe-drilling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/lathe-drilling.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p>The mill was then used to flatten a bottom section on the player models to create an optimal contact surface with the game ball. It was also used to create the goal cavity in the goal walls, as well as ensure that all walls are of equal height.</p> <swiper-container class="media-swiper" keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <video src="/assets/img/mill_player.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/foosball-walls-480.webp 480w,/assets/img/foosball-walls-800.webp 800w,/assets/img/foosball-walls-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/foosball-walls.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <video src="/assets/img/mill_goal.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </swiper-slide> </swiper-container> <p>Of all machined components, the part that differs the most from the original concept is the player base, due to the complexity of the first iteration. The finalized geometry reflects a simpler design based on the amount of time available to complete the machining and the resources and machines available. Because of how each individual component meticulously interfaces, system-level assembly was especially important. Precision was emphasized as the assembly phase began, where threaded inserts were carefully installed to ensure the walls, base, and player rods connected as desired. Bushings were ordered from McMaster-Carr so that the rods could translate with little friction in the side walls.</p> <swiper-container class="media-swiper" keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true"> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wall-player-assembly-480.webp 480w,/assets/img/wall-player-assembly-800.webp 800w,/assets/img/wall-player-assembly-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wall-player-assembly.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wall-bushings-480.webp 480w,/assets/img/wall-bushings-800.webp 800w,/assets/img/wall-bushings-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/wall-bushings.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/final-assembly-480.webp 480w,/assets/img/final-assembly-800.webp 800w,/assets/img/final-assembly-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/final-assembly.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> </swiper-container> <p>For the most part, the mini foosball table functions as desired. However, due to the changes in the player model geometries from CAD to real life, there is slight unwanted contact that occurs between players of consecutive rods, which can significantly affect performance.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Design, manufacturing, and assembly of a scaled-down foosball table]]></summary></entry></feed>